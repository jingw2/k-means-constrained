
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Welcome to k-means-constrained’s documentation! &#8212; k-means-constrained 0.0.2 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/language_data.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="module-k_means_constrained">
<span id="welcome-to-k-means-constrained-s-documentation"></span><h1>Welcome to k-means-constrained’s documentation!<a class="headerlink" href="#module-k_means_constrained" title="Permalink to this headline">¶</a></h1>
<dl class="class">
<dt id="k_means_constrained.KMeansConstrained">
<em class="property">class </em><code class="sig-prename descclassname">k_means_constrained.</code><code class="sig-name descname">KMeansConstrained</code><span class="sig-paren">(</span><em class="sig-param">n_clusters=8</em>, <em class="sig-param">size_min=None</em>, <em class="sig-param">size_max=None</em>, <em class="sig-param">init='k-means++'</em>, <em class="sig-param">n_init=10</em>, <em class="sig-param">max_iter=300</em>, <em class="sig-param">tol=0.0001</em>, <em class="sig-param">verbose=False</em>, <em class="sig-param">random_state=None</em>, <em class="sig-param">copy_x=True</em>, <em class="sig-param">n_jobs=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/k_means_constrained/k_means_constrained_.html#KMeansConstrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#k_means_constrained.KMeansConstrained" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">k_means_constrained.sklearn_import.cluster.k_means_.KMeans</span></code></p>
<p>K-Means clustering with minimum and maximum cluster size constraints</p>
<dl>
<dt>n_clusters<span class="classifier">int, optional, default: 8</span></dt><dd><p>The number of clusters to form as well as the number of
centroids to generate.</p>
</dd>
<dt>size_min<span class="classifier">int, optional, default: None</span></dt><dd><p>Constrain the label assignment so that each cluster has a minimum
size of size_min. If None, no constrains will be applied</p>
</dd>
<dt>size_max<span class="classifier">int, optional, default: None</span></dt><dd><p>Constrain the label assignment so that each cluster has a maximum
size of size_max. If None, no constrains will be applied</p>
</dd>
<dt>init<span class="classifier">{‘k-means++’, ‘random’ or an ndarray}</span></dt><dd><p>Method for initialization, defaults to ‘k-means++’:</p>
<p>‘k-means++’ : selects initial cluster centers for k-mean
clustering in a smart way to speed up convergence. See section
Notes in k_init for more details.</p>
<p>‘random’: choose k observations (rows) at random from data for
the initial centroids.</p>
<p>If an ndarray is passed, it should be of shape (n_clusters, n_features)
and gives the initial centers.</p>
</dd>
<dt>n_init<span class="classifier">int, default: 10</span></dt><dd><p>Number of times the k-means algorithm will be run with different
centroid seeds. The final results will be the best output of
n_init consecutive runs in terms of inertia.</p>
</dd>
<dt>max_iter<span class="classifier">int, default: 300</span></dt><dd><p>Maximum number of iterations of the k-means algorithm for a
single run.</p>
</dd>
<dt>tol<span class="classifier">float, default: 1e-4</span></dt><dd><p>Relative tolerance with regards to inertia to declare convergence</p>
</dd>
<dt>verbose<span class="classifier">int, default 0</span></dt><dd><p>Verbosity mode.</p>
</dd>
<dt>random_state<span class="classifier">int, RandomState instance or None, optional, default: None</span></dt><dd><p>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</p>
</dd>
<dt>copy_x<span class="classifier">boolean, default True</span></dt><dd><p>When pre-computing distances it is more numerically accurate to center
the data first.  If copy_x is True, then the original data is not
modified.  If False, the original data is modified, and put back before
the function returns, but small numerical differences may be introduced
by subtracting and then adding the data mean.</p>
</dd>
<dt>n_jobs<span class="classifier">int</span></dt><dd><p>The number of jobs to use for the computation. This works by computing
each of the n_init runs in parallel.</p>
<p>If -1 all CPUs are used. If 1 is given, no parallel computing code is
used at all, which is useful for debugging. For n_jobs below -1,
(n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one
are used.</p>
</dd>
</dl>
<dl class="simple">
<dt><a href="#id1"><span class="problematic" id="id2">cluster_centers_</span></a><span class="classifier">array, [n_clusters, n_features]</span></dt><dd><p>Coordinates of cluster centers</p>
</dd>
<dt><a href="#id3"><span class="problematic" id="id4">labels_</span></a> :</dt><dd><p>Labels of each point</p>
</dd>
<dt><a href="#id5"><span class="problematic" id="id6">inertia_</span></a><span class="classifier">float</span></dt><dd><p>Sum of squared distances of samples to their closest cluster center.</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">k_means_constrained</span> <span class="k">import</span> <span class="n">KMeansConstrained</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="gp">... </span>              <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">KMeansConstrained</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">size_min</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">size_max</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">labels_</span>
<span class="go">array([0, 0, 0, 1, 1, 1], dtype=int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="go">array([0, 1], dtype=int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">cluster_centers_</span>
<span class="go">array([[ 1.,  2.],</span>
<span class="go">       [ 4.,  2.]])</span>
</pre></div>
</div>
<p>K-means problem constrained with a minimum and/or maximum size for each cluster.</p>
<p>The constrained assignment is formulated as a Minimum Cost Flow (MCF) linear network optimisation
problem. This is then solved using a cost-scaling push-relabel algorithm. The implementation used is</p>
<blockquote>
<div><p>Google’s Operations Research tools’s <cite>SimpleMinCostFlow</cite>.</p>
</div></blockquote>
<p>Ref:
1. Bradley, P. S., K. P. Bennett, and Ayhan Demiriz. “Constrained k-means clustering.”</p>
<blockquote>
<div><p>Microsoft Research, Redmond (2000): 1-8.</p>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><dl class="simple">
<dt>Google’s SimpleMinCostFlow implementation:</dt><dd><p><a class="reference external" href="https://github.com/google/or-tools/blob/master/ortools/graph/min_cost_flow.h">https://github.com/google/or-tools/blob/master/ortools/graph/min_cost_flow.h</a></p>
</dd>
</dl>
</li>
</ol>
<dl class="method">
<dt id="k_means_constrained.KMeansConstrained.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">X</em>, <em class="sig-param">y=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/k_means_constrained/k_means_constrained_.html#KMeansConstrained.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#k_means_constrained.KMeansConstrained.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute k-means clustering.</p>
<dl class="simple">
<dt>X<span class="classifier">array-like, shape=(n_samples, n_features)</span></dt><dd><p>Training instances to cluster.</p>
</dd>
</dl>
<p>y : Ignored</p>
</dd></dl>

<dl class="method">
<dt id="k_means_constrained.KMeansConstrained.fit_predict">
<code class="sig-name descname">fit_predict</code><span class="sig-paren">(</span><em class="sig-param">X</em>, <em class="sig-param">y=None</em><span class="sig-paren">)</span><a class="headerlink" href="#k_means_constrained.KMeansConstrained.fit_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute cluster centers and predict cluster index for each sample.</p>
<p>Convenience method; equivalent to calling fit(X) followed by
predict(X).</p>
<dl class="simple">
<dt>X<span class="classifier">{array-like, sparse matrix}, shape = [n_samples, n_features]</span></dt><dd><p>New data to transform.</p>
</dd>
</dl>
<dl class="simple">
<dt>labels<span class="classifier">array, shape [n_samples,]</span></dt><dd><p>Index of the cluster each sample belongs to.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="k_means_constrained.KMeansConstrained.fit_transform">
<code class="sig-name descname">fit_transform</code><span class="sig-paren">(</span><em class="sig-param">X</em>, <em class="sig-param">y=None</em><span class="sig-paren">)</span><a class="headerlink" href="#k_means_constrained.KMeansConstrained.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute clustering and transform X to cluster-distance space.</p>
<p>Equivalent to fit(X).transform(X), but more efficiently implemented.</p>
<dl class="simple">
<dt>X<span class="classifier">{array-like, sparse matrix}, shape = [n_samples, n_features]</span></dt><dd><p>New data to transform.</p>
</dd>
</dl>
<dl class="simple">
<dt>X_new<span class="classifier">array, shape [n_samples, k]</span></dt><dd><p>X transformed in the new space.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="k_means_constrained.KMeansConstrained.get_params">
<code class="sig-name descname">get_params</code><span class="sig-paren">(</span><em class="sig-param">deep=True</em><span class="sig-paren">)</span><a class="headerlink" href="#k_means_constrained.KMeansConstrained.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get parameters for this estimator.</p>
<dl class="simple">
<dt>deep<span class="classifier">boolean, optional</span></dt><dd><p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</dd>
</dl>
<dl class="simple">
<dt>params<span class="classifier">mapping of string to any</span></dt><dd><p>Parameter names mapped to their values.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="k_means_constrained.KMeansConstrained.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#k_means_constrained.KMeansConstrained.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict the closest cluster each sample in X belongs to.</p>
<p>In the vector quantization literature, <cite>cluster_centers_</cite> is called
the code book and each value returned by <cite>predict</cite> is the index of
the closest code in the code book.</p>
<dl class="simple">
<dt>X<span class="classifier">{array-like, sparse matrix}, shape = [n_samples, n_features]</span></dt><dd><p>New data to predict.</p>
</dd>
</dl>
<dl class="simple">
<dt>labels<span class="classifier">array, shape [n_samples,]</span></dt><dd><p>Index of the cluster each sample belongs to.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="k_means_constrained.KMeansConstrained.score">
<code class="sig-name descname">score</code><span class="sig-paren">(</span><em class="sig-param">X</em>, <em class="sig-param">y=None</em><span class="sig-paren">)</span><a class="headerlink" href="#k_means_constrained.KMeansConstrained.score" title="Permalink to this definition">¶</a></dt>
<dd><p>Opposite of the value of X on the K-means objective.</p>
<dl class="simple">
<dt>X<span class="classifier">{array-like, sparse matrix}, shape = [n_samples, n_features]</span></dt><dd><p>New data.</p>
</dd>
</dl>
<dl class="simple">
<dt>score<span class="classifier">float</span></dt><dd><p>Opposite of the value of X on the K-means objective.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="k_means_constrained.KMeansConstrained.set_params">
<code class="sig-name descname">set_params</code><span class="sig-paren">(</span><em class="sig-param">**params</em><span class="sig-paren">)</span><a class="headerlink" href="#k_means_constrained.KMeansConstrained.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the parameters of this estimator.</p>
<p>The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code class="docutils literal notranslate"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></code> so that it’s possible to update each
component of a nested object.</p>
<p>self</p>
</dd></dl>

<dl class="method">
<dt id="k_means_constrained.KMeansConstrained.transform">
<code class="sig-name descname">transform</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="headerlink" href="#k_means_constrained.KMeansConstrained.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform X to a cluster-distance space.</p>
<p>In the new space, each dimension is the distance to the cluster
centers.  Note that even if X is sparse, the array returned by
<cite>transform</cite> will typically be dense.</p>
<dl class="simple">
<dt>X<span class="classifier">{array-like, sparse matrix}, shape = [n_samples, n_features]</span></dt><dd><p>New data to transform.</p>
</dd>
</dl>
<dl class="simple">
<dt>X_new<span class="classifier">array, shape [n_samples, k]</span></dt><dd><p>X transformed in the new space.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="#">k-means-constrained</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="#">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2020, Josh Levy-Kramer.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 2.2.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/index.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>